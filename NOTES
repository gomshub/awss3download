NOTES:

python
import boto3

def sync_s3_subfolders(source_bucket, source_folder, destination_bucket, destination_folder):
    s3_client = boto3.client('s3')

    # Retrieve the list of objects in the source subfolder
    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=source_folder)
    source_objects = response['Contents']

    # Iterate through the objects and copy them to the destination subfolder
    for obj in source_objects:
        key = obj['Key']
        destination_key = key.replace(source_folder, destination_folder, 1)  # Replace source folder with destination folder
        copy_source = {'Bucket': source_bucket, 'Key': key}
        s3_client.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=destination_key)

    print("Data synchronization completed.")

# Usage: Provide the names of the source and destination buckets, along with their subfolder paths
source_bucket_name = 'your-source-bucket'
source_folder_name = 'your-source-subfolder/'
destination_bucket_name = 'your-destination-bucket'
destination_folder_name = 'your-destination-subfolder/'

sync_s3_subfolders(source_bucket_name, source_folder_name, destination_bucket_name, destination_folder_name)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
python
import os
import boto3

def upload_files_to_s3(bucket_name, directory_path):
    s3_client = boto3.client('s3')

    for root, dirs, files in os.walk(directory_path):
        for file in files:
            file_path = os.path.join(root, file)
            s3_key = os.path.relpath(file_path, directory_path)
            s3_client.upload_file(file_path, bucket_name, s3_key)

# Example usage
bucket_name = 'your-bucket-name'
directory_path = '/path/to/your/directory'

upload_files_to_s3(bucket_name, directory_path)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
python
import os
import datetime
import boto3

def create_subfolder_with_date_format(bucket_name, file_path):
    s3_client = boto3.client('s3')
    
    # Get the file's modification timestamp
    file_timestamp = os.path.getmtime(file_path)
    
    # Convert the timestamp to datetime object
    file_datetime = datetime.datetime.fromtimestamp(file_timestamp)
    
    # Generate the desired date format
    date_format = file_datetime.strftime('%Y-%m-%d')
    
    # Create the subfolder using the date format
    subfolder_path = f'{date_format}/'
    s3_client.put_object(Bucket=bucket_name, Key=subfolder_path)
    
# Example usage
bucket_name = 'your-bucket-name'
file_path = '/path/to/your/file.txt'

create_subfolder_with_date_format(bucket_name, file_path)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
python
import pyarrow.parquet as pq

# Define the path to the input Parquet file
input_file = 'input.parquet'

# Define the path to the output Parquet file
output_file = 'output.parquet'

# Define the value you want to lookup
lookup_value = 'your_lookup_value'

# Read the input Parquet file
table = pq.read_table(input_file)

# Convert the Parquet table to a Pandas DataFrame
df = table.to_pandas()

# Filter the DataFrame based on the lookup value
matched_rows = df[df['column_name'] == lookup_value]

# Convert the filtered DataFrame back to a PyArrow Table
output_table = pa.Table.from_pandas(matched_rows)

# Write the output table to the output Parquet file
pq.write_table(output_table, output_file)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
To iterate and synchronize more than 1000 objects using a paginator from one S3 subfolder to another S3 subfolder using `boto3` in Python, you can follow these steps:

1. Import the necessary libraries:

```python
import boto3
```

2. Create an S3 client:

```python
s3 = boto3.client('s3')
```

3. Use the `list_objects_v2` method to get a list of all objects in the source subfolder. Set the `Prefix` parameter to the source subfolder name:

```python
response = s3.list_objects_v2(Bucket='your_bucket_name', Prefix='source_subfolder/')
```

4. Check if the response contains any objects. If the response has more than 1000 objects, it means there are more objects that need to be paginated. Otherwise, you can proceed to the next step:

```python
if 'Contents' in response:
    objects = response['Contents']
    if response['KeyCount'] > 1000:
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket='your_bucket_name', Prefix='source_subfolder/')
        for page in pages:
            objects.extend(page['Contents'])
```

5. Iterate through each object and copy it to the destination subfolder using the `copy_object` method:

```python
for obj in objects:
    source_key = obj['Key']
    destination_key = source_key.replace('source_subfolder/', 'destination_subfolder/')
    s3.copy_object(Bucket='your_bucket_name', CopySource={'Bucket': 'your_bucket_name', 'Key': source_key}, Key=destination_key)
```

Note: Replace `your_bucket_name`, `source_subfolder`, and `destination_subfolder` with your actual bucket name, source subfolder, and destination subfolder, respectively.

This code will iterate and synchronize more than 1000 objects from one S3 subfolder to another S3 subfolder using the `boto3` library in Python.


python
import boto3

s3 = boto3.client('s3')

def list_subfolders(bucket_name, prefix):
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')
    subfolders = [common_prefix['Prefix'].split('/')[0] for common_prefix in response.get('CommonPrefixes', [])]
    return subfolders

bucket_name = 'your_bucket_name'
prefix = 'your_folder_prefix/'

subfolders = list_subfolders(bucket_name, prefix)
print(subfolders)


python
import boto3

s3 = boto3.client('s3')

def list_subfolders(bucket_name, prefix):
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')

    subfolders = []
    for common_prefix in response.get('CommonPrefixes', []):
        subfolder_path = common_prefix['Prefix']
        subfolder_name = subfolder_path.strip('/').split('/')[-1]
        subfolders.append(subfolder_name)

    return subfolders

bucket_name = 'your_bucket_name'
prefix = 'your_parent_folder/subfolder/'

subfolders = list_subfolders(bucket_name, prefix)
print(subfolders)



