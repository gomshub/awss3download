NOTES:
Certainly! Here's a sample shell script that recursively searches for zip files in a directory, identifies the files from the previous date, extracts them, and fetches the data within:

```shell
#!/bin/bash

# Specify the directory to search
directory="/path/to/directory"

# Get the previous date in YYYY-MM-DD format
previous_date=$(date -d "yesterday" +%Y-%m-%d)

# Find zip files from the previous date recursively
files=$(find "$directory" -type f -name "*.zip" -newermt "$previous_date" ! -newermt "$previous_date + 1 day")

# Loop through each zip file
for file in $files; do
  # Extract the file to a temporary directory
  temp_dir=$(mktemp -d)
  unzip "$file" -d "$temp_dir"

  # Fetch and process the data
  # Replace the echo statement with your desired data processing logic
  echo "Fetching data from $file"
  cat "$temp_dir/data.txt"  # Example: Assuming the data is stored in a file called data.txt

  # Clean up the temporary directory
  rm -rf "$temp_dir"
done
```

Make sure to replace `/path/to/directory` with the actual directory path you want to search. Additionally, modify the data processing logic according to your specific requirements.







====#
python
import boto3

def sync_s3_subfolders(source_bucket, source_folder, destination_bucket, destination_folder):
    s3_client = boto3.client('s3')

    # Retrieve the list of objects in the source subfolder
    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=source_folder)
    source_objects = response['Contents']

    # Iterate through the objects and copy them to the destination subfolder
    for obj in source_objects:
        key = obj['Key']
        destination_key = key.replace(source_folder, destination_folder, 1)  # Replace source folder with destination folder
        copy_source = {'Bucket': source_bucket, 'Key': key}
        s3_client.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=destination_key)

    print("Data synchronization completed.")

# Usage: Provide the names of the source and destination buckets, along with their subfolder paths
source_bucket_name = 'your-source-bucket'
source_folder_name = 'your-source-subfolder/'
destination_bucket_name = 'your-destination-bucket'
destination_folder_name = 'your-destination-subfolder/'

sync_s3_subfolders(source_bucket_name, source_folder_name, destination_bucket_name, destination_folder_name)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
python
import os
import boto3

def upload_files_to_s3(bucket_name, directory_path):
    s3_client = boto3.client('s3')

    for root, dirs, files in os.walk(directory_path):
        for file in files:
            file_path = os.path.join(root, file)
            s3_key = os.path.relpath(file_path, directory_path)
            s3_client.upload_file(file_path, bucket_name, s3_key)

# Example usage
bucket_name = 'your-bucket-name'
directory_path = '/path/to/your/directory'

upload_files_to_s3(bucket_name, directory_path)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
python
import os
import datetime
import boto3

def create_subfolder_with_date_format(bucket_name, file_path):
    s3_client = boto3.client('s3')
    
    # Get the file's modification timestamp
    file_timestamp = os.path.getmtime(file_path)
    
    # Convert the timestamp to datetime object
    file_datetime = datetime.datetime.fromtimestamp(file_timestamp)
    
    # Generate the desired date format
    date_format = file_datetime.strftime('%Y-%m-%d')
    
    # Create the subfolder using the date format
    subfolder_path = f'{date_format}/'
    s3_client.put_object(Bucket=bucket_name, Key=subfolder_path)
    
# Example usage
bucket_name = 'your-bucket-name'
file_path = '/path/to/your/file.txt'

create_subfolder_with_date_format(bucket_name, file_path)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
python
import pyarrow.parquet as pq

# Define the path to the input Parquet file
input_file = 'input.parquet'

# Define the path to the output Parquet file
output_file = 'output.parquet'

# Define the value you want to lookup
lookup_value = 'your_lookup_value'

# Read the input Parquet file
table = pq.read_table(input_file)

# Convert the Parquet table to a Pandas DataFrame
df = table.to_pandas()

# Filter the DataFrame based on the lookup value
matched_rows = df[df['column_name'] == lookup_value]

# Convert the filtered DataFrame back to a PyArrow Table
output_table = pa.Table.from_pandas(matched_rows)

# Write the output table to the output Parquet file
pq.write_table(output_table, output_file)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
To iterate and synchronize more than 1000 objects using a paginator from one S3 subfolder to another S3 subfolder using `boto3` in Python, you can follow these steps:

1. Import the necessary libraries:

```python
import boto3
```

2. Create an S3 client:

```python
s3 = boto3.client('s3')
```

3. Use the `list_objects_v2` method to get a list of all objects in the source subfolder. Set the `Prefix` parameter to the source subfolder name:

```python
response = s3.list_objects_v2(Bucket='your_bucket_name', Prefix='source_subfolder/')
```

4. Check if the response contains any objects. If the response has more than 1000 objects, it means there are more objects that need to be paginated. Otherwise, you can proceed to the next step:

```python
if 'Contents' in response:
    objects = response['Contents']
    if response['KeyCount'] > 1000:
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket='your_bucket_name', Prefix='source_subfolder/')
        for page in pages:
            objects.extend(page['Contents'])
```

5. Iterate through each object and copy it to the destination subfolder using the `copy_object` method:

```python
for obj in objects:
    source_key = obj['Key']
    destination_key = source_key.replace('source_subfolder/', 'destination_subfolder/')
    s3.copy_object(Bucket='your_bucket_name', CopySource={'Bucket': 'your_bucket_name', 'Key': source_key}, Key=destination_key)
```

Note: Replace `your_bucket_name`, `source_subfolder`, and `destination_subfolder` with your actual bucket name, source subfolder, and destination subfolder, respectively.

This code will iterate and synchronize more than 1000 objects from one S3 subfolder to another S3 subfolder using the `boto3` library in Python.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------7777

python
import boto3

s3 = boto3.client('s3')

def list_subfolders(bucket_name, prefix):
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')
    subfolders = [common_prefix['Prefix'].split('/')[0] for common_prefix in response.get('CommonPrefixes', [])]
    return subfolders

bucket_name = 'your_bucket_name'
prefix = 'your_folder_prefix/'

subfolders = list_subfolders(bucket_name, prefix)
print(subfolders)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

python
import boto3

s3 = boto3.client('s3')

def list_subfolders(bucket_name, prefix):
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')

    subfolders = []
    for common_prefix in response.get('CommonPrefixes', []):
        subfolder_path = common_prefix['Prefix']
        subfolder_name = subfolder_path.strip('/').split('/')[-1]
        subfolders.append(subfolder_name)

    return subfolders

bucket_name = 'your_bucket_name'
prefix = 'your_parent_folder/subfolder/'

subfolders = list_subfolders(bucket_name, prefix)
print(subfolders)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
bash
#!/bin/bash

# Set the directory to list files from
directory="/path/to/directory"

# Iterate over files in the directory
for file in "$directory"/*
do
  # Check if the file is a regular file
  if [ -f "$file" ]
  then
    # Get the file's generated date
    generated_date=$(stat -c %y "$file" | awk '{print $1}')
    
    # Print the file name and generated date
    echo "File: $file, Generated Date: $generated_date"
  fi
done

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
python
pip install pyarrow
pip install s3fs

python
import pyarrow.parquet as pq
import pyarrow as pa
import s3fs


python
bucket_name = 'your_bucket_name'
folder_paths = ['folder1/', 'folder2/', ...]  # Add the desired folder paths

python
lookup_value = 'your_lookup_value'

python
matched_data = []

python
s3 = s3fs.S3FileSystem()

for folder_path in folder_paths:
    full_path = f'{bucket_name}/{folder_path}'
    files = s3.glob(f'{full_path}*.parquet')
    
    for file in files:
        with s3.open(file, 'rb') as f:
            table = pq.read_table(f)
            df = table.to_pandas()
            
            # Perform the lookup on the desired field
            matched_df = df[df['field_name'] == lookup_value]
            
            # Append the matched data
            matched_data.append(matched_df)


python
output_df = pd.concat(matched_data)

python
output_table = pa.Table.from_pandas(output_df)
pq.write_table(output_table, 'output.parquet')

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
===============================================

python
import boto3
from concurrent.futures import ThreadPoolExecutor
python
s3 = boto3.client('s3')

python
def upload_file(file_path, bucket_name, key_prefix):
    key = key_prefix + '/' + file_path.split('/')[-1]
    s3.upload_file(file_path, bucket_name, key)


python
with ThreadPoolExecutor(max_workers=num_threads) as executor:
    for file_path in file_paths:
        executor.submit(upload_file, file_path, bucket_name, key_prefix)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
================#

python
import boto3
import pyarrow.parquet as pq
import pyarrow as pa

python
s3 = boto3.client('s3')

python
def process_parquet_file(file_path):
    # Download the Parquet file from S3
    s3.download_file(bucket_name, file_path, 'temp.parquet')

    # Read the Parquet file into a Pandas DataFrame
    df = pq.read_table('temp.parquet').to_pandas()

    # Perform the lookup on the desired field
    matched_df = df[df['field_name'] == 'lookup_value']

    return matched_df


python
matched_dfs = []
for file_path in file_paths:
    matched_df = process_parquet_file(file_path)
    matched_dfs.append(matched_df)


python
output_df = pd.concat(matched_dfs)


python
output_table = pa.Table.from_pandas(output_df)
pq.write_table(output_table, 'output.parquet')


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

python
import os
import boto3
from datetime import datetime

# Specify the directory path
directory = '/path/to/directory/'

# Connect to S3
s3 = boto3.client('s3')

# Get the current month
current_month = datetime.now().strftime('%m')

# Iterate through files in the directory
for filename in os.listdir(directory):
    file_path = os.path.join(directory, filename)
    
    # Check if the file's last modified month matches the current month
    if datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%m') == current_month:
        
        # Upload the file to S3
        s3.upload_file(file_path, 'your-bucket-name', filename)
        print(f"Uploaded {filename} to S3.")


------------â€”


python
import os
import boto3
from concurrent.futures import ThreadPoolExecutor

# Set your AWS credentials and region
aws_access_key_id = 'your_access_key_id'
aws_secret_access_key = 'your_secret_access_key'
region_name = 'your_region_name'

# Set the S3 bucket name and folder path
bucket_name = 'your_bucket_name'
folder_path = 'your_folder_path'

# Set the maximum number of workers (threads)
max_workers = 10

# Create an S3 client
s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)


def upload_file(file_path):
    # Upload a file to S3
    key = os.path.join(folder_path, os.path.basename(file_path))
    s3.upload_file(file_path, bucket_name, key)
    print(f"Uploaded {file_path} to S3")


# Get a list of files in the folder
folder = 'your_folder_path'
file_paths = []
for root, dirs, files in os.walk(folder):
    for file in files:
        file_paths.append(os.path.join(root, file))

# Create a ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=max_workers) as executor:
    # Submit the upload tasks to the executor
    for file_path in file_paths:
        executor.submit(upload_file, file_path)

# All tasks have been submitted, now wait for them to complete
# The ThreadPoolExecutor automatically manages the worker threads

print("All files uploaded!")


-----------

python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import boto3


text_data = ['Line 1', 'Line 2', 'Line 3']  # Replace with your actual text data
df = pd.DataFrame(text_data, columns=['TextColumn'])
python
table = pa.Table.from_pandas(df)

python
parquet_buffer = pa.BufferOutputStream()

python
pq.write_table(table, parquet_buffer)

python
s3_bucket_name = 'your_s3_bucket_name'
s3_file_path = 's3_folder/parquet_file.parquet'
s3_client = boto3.client('s3')
parquet_buffer.seek(0)  # Reset the buffer position
s3_client.upload_fileobj(parquet_buffer, s3_bucket_name, s3_file_path)


-----------
python
import os
import zipfile
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import boto3
from multiprocessing import Pool

def extract_and_process_zip(zip_file):
    extract_folder_path = 'path_to_extract_folder'  # Replace with your desired extract folder path
    zip_file_path = os.path.join('path_to_zip_files', zip_file)  # Replace with the path to your zip files directory
    
    # Extract the zip file
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_folder_path)
    
    # Read the data from the extracted file
    extracted_file_path = os.path.join(extract_folder_path, 'extracted_file.txt')  # Replace with the actual extracted file path
    df = pd.read_csv(extracted_file_path)  # Replace with the appropriate method to read your extracted data
    
    # Convert DataFrame to PyArrow Table
    table = pa.Table.from_pandas(df)
    
    # Write Parquet data to in-memory buffer
    parquet_buffer = pa.BufferOutputStream()
    pq.write_table(table, parquet_buffer)
    
    # Upload Parquet data to S3
    s3_bucket_name = 'your_s3_bucket_name'
    s3_file_path = f's3_folder/{zip_file}.parquet'
    s3_client = boto3.client('s3')
    
    parquet_buffer.seek(0)  # Reset the buffer position
    s3_client.upload_fileobj(parquet_buffer, s3_bucket_name, s3_file_path)
    
    # Clean up extracted files if needed
    os.remove(extracted_file_path)

# Get a list of zip files in the directory
zip_files_directory = 'path_to_zip_files_directory'  # Replace with the path to your zip files directory
zip_files = [f for f in os.listdir(zip_files_directory) if f.endswith('.zip')]

# Process zip files in parallel
num_processes = min(len(zip_files), os.cpu_count())  # Adjust the number of processes as per your system's capacity
with Pool(num_processes) as pool:
    pool.map(extract_and_process_zip, zip_files)


-------

python
import zipfile
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import boto3
from multiprocessing import Pool
import io

def process_zip(zip_file):
    s3_bucket_name = 'your_s3_bucket_name'
    s3_folder = 's3_folder'
    s3_client = boto3.client('s3')

    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        for file_name in zip_ref.namelist():
            if file_name.endswith('.txt'):
                with zip_ref.open(file_name, 'r') as file:
                    # Read the data from the file within the zip
                    data = file.read().decode('utf-8')
                    
                    # Process the data (example: convert to DataFrame)
                    df = pd.DataFrame({'Data': [data]})
                    
                    # Convert DataFrame to PyArrow Table
                    table = pa.Table.from_pandas(df)
                    
                    # Write Parquet data to in-memory buffer
                    parquet_buffer = io.BytesIO()
                    pq.write_table(table, parquet_buffer)
                    
                    # Upload Parquet data to S3
                    s3_file_path = f'{s3_folder}/{file_name[:-4]}.parquet'
                    parquet_buffer.seek(0)  # Reset the buffer position
                    s3_client.upload_fileobj(parquet_buffer, s3_bucket_name, s3_file_path)

# List of zip file paths
zip_files = ['path_to_zip_file1.zip', 'path_to_zip_file2.zip', ...]  # Replace with the actual zip file paths

# Process zip files in parallel
num_processes = min(len(zip_files), os.cpu_count())  # Adjust the number of processes as per your system's capacity
with Pool(num_processes) as pool:
    pool.map(process_zip, zip_files)


========#

Certainly! Here's an updated version of the code that unzips the gz file, untars the resulting file to a buffer, reads the buffer data as a txt file, and converts the txt data to a Parquet buffer using Python:

```python
import gzip
import tarfile
import io
import pandas as pd
import pyarrow as pa

# Define the path to your gz file
gz_file_path = "path/to/your/file.tar.gz"

# Unzip the gz file
with gzip.open(gz_file_path, 'rb') as gz_file:
    tar_buffer = io.BytesIO(gz_file.read())

# Extract the tar file from the buffer
with tarfile.open(fileobj=tar_buffer, mode='r') as tar:
    txt_file = tar.extractfile('data.txt')
    txt_data = txt_file.read().decode()

# Convert text data to a DataFrame (assuming it's in CSV format)
df = pd.read_csv(io.StringIO(txt_data))

# Write the DataFrame to a Parquet buffer
parquet_buffer = io.BytesIO()
table = pa.Table.from_pandas(df)
pq.write_table(table, parquet_buffer)

# Reset the buffer position to the beginning
parquet_buffer.seek(0)

# You can now use the parquet_buffer as needed
```

Make sure to replace `"path/to/your/file.tar.gz"` with the actual path to your gz file. This code unzips the gz file, untars the resulting file to a buffer, reads the buffer data as a txt file, converts it to a pandas DataFrame, and finally writes the DataFrame to a Parquet buffer named `parquet_buffer`. You can use `parquet_buffer` as needed for further processing or saving it to a file if desired.


## tar file name

python
import gzip
import tarfile
import io

# Define the path to your gz file
gz_file_path = "path/to/your/file.tar.gz"

# Unzip the gz file
with gzip.open(gz_file_path, 'rb') as gz_file:
    tar_buffer = io.BytesIO(gz_file.read())

# Extract the files from the tar buffer
with tarfile.open(fileobj=tar_buffer, mode='r') as tar:
    for tarinfo in tar:
        if tarinfo.isfile():
            # Extract the file to a buffer
            file_buffer = tar.extractfile(tarinfo).read()
            # Get the filename of the extracted file
            filename = tarinfo.name
            
            # Process the file or perform any desired operations
            print("Extracted file:", filename)
            print("File contents:", file_buffer)

            # You can now use the filename and file_buffer as needed

=========#

read multiple parquet files in a s3 folder as txt data and return as gz file using Python

To read multiple Parquet files from an S3 folder as text data and return them as a gzipped file using Python, you can utilize the `boto3` library for S3 operations and the `pyarrow` library for reading and converting Parquet files. Here's an example code snippet:

```python
import boto3
import pyarrow.parquet as pq
import gzip
import io

# Define your S3 bucket and folder path
bucket_name = "your_bucket_name"
folder_path = "your_folder_path"

# Initialize the S3 client
s3_client = boto3.client('s3')

# List all Parquet files in the S3 folder
response = s3_client.list_objects(Bucket=bucket_name, Prefix=folder_path)
parquet_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]

# Read and concatenate the Parquet files as text data
txt_data = ""
for parquet_file in parquet_files:
    # Read the Parquet file
    obj = s3_client.get_object(Bucket=bucket_name, Key=parquet_file)
    parquet_buffer = io.BytesIO(obj['Body'].read())
    table = pq.read_table(parquet_buffer)
    
    # Convert to DataFrame and concatenate the text data
    df = table.to_pandas()
    txt_data += df.to_string(index=False)

# Compress the text data as gzipped file
gz_buffer = io.BytesIO()
with gzip.GzipFile(fileobj=gz_buffer, mode='w') as gz_file:
    gz_file.write(txt_data.encode('utf-8'))

# Upload the gzipped file to S3
s3_client.put_object(Body=gz_buffer.getvalue(), Bucket=bucket_name, Key="output/file.tar.gz")
```

Make sure to replace `"your_bucket_name"` with the name of your S3 bucket and `"your_folder_path"` with the path to your Parquet files within the bucket. The code lists all the Parquet files in the specified S3 folder, reads and concatenates them as text data, compresses the text data as a gzipped file, and uploads it back to S3 with the specified output file path.
